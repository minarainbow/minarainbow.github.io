---
---


@inproceedings{chang2021rubyslippers,
  abbr={CHI},
  title={RubySlippers: Supporting Content-based Voice Navigation for How-to Videos},
  author={Chang, Minsuk and Huh, Mina and Kim, Juho},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021},
  abstract={Directly manipulating the timeline, such as scrubbing for thumbnails, is the standard way of controlling how-to videos. However, when how-to videos involve physical activities, people inconveniently alternate between controlling the video and performing the tasks. Adopting a voice user interface allows people to control the video with voice while performing the tasks with hands. However, naively translating timeline manipulation into voice user interfaces (VUI) results in temporal referencing (e.g.  ``rewind 20 seconds''), which requires a different mental model for navigation and thereby limiting users' ability to peek into the content. We present RubySlippers, a system that supports efficient content-based voice navigation through keyword-based queries. Our computational pipeline automatically detects referenceable elements in the video, and finds the video segmentation that minimizes the number of needed navigational commands. Our evaluation (N=12) shows that participants could perform three representative navigation tasks with fewer commands and less frustration using RubySlippers than the conventional voice-enabled video interface.},
  pdf={https://kixlab.github.io/website-files/2021/chi2021-RubySlippers-paper.pdf},
  html={https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445131},
  selected={true},
}


@inproceedings{huh2022cocomix,
  abbr={CHI},
  title={{Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility}},
  author={Huh, Mina and Lee, Yunjung and Choi, Dasom and Kim, Haesoo and Oh, Uran and Kim, Juho},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  year={2022},
  abstract={Webtoon is a type of digital comics read online where readers can leave comments to share their thoughts on the story. While it has experienced a surge in popularity internationally, people with visual impairments cannot enjoy webtoon with the lack of an accessible format. While traditional image description practices can be adopted, resulting descriptions cannot preserve webtoons' unique values such as control over the reading pace and social engagement through comments. To improve the webtoon reading experience for BLV users, we propose Cocomix, an interactive webtoon-reader that leverages comments into the design of novel webtoon interactions. Since comments can identify story highlights and provide additional context, we designed a system that provides 1) comments-based adaptive description with selective access to details and 2) panel-anchored comments for easy access to relevant descriptive comments. Our evaluation (N=12) showed that Cocomix users could adapt the description for various needs and better utilize comments.},
  pdf={CHI2022_Cocomix.pdf},
  html={https://dl.acm.org/doi/fullHtml/10.1145/3491102.3502081},
  selected={true},
}

@inproceedings{huh2022makeup,
  abbr={CHI},
  title={{``It Feels Like Taking a Gamble'': Exploring Perceptions, Practices, and Challenges of Using Makeup and Cosmetics for People with Visual Impairments}},
  author={ Li*, Mingzhe and Spektor*, Francheska and Xia*, Meng and Huh*, Mina and Cederberg, Peter and Gong, Yuqi and Shinohara, Kristen and Carrington, Patrick},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  year={2022},
  abstract={Makeup and cosmetics offer the potential for self-reinvention and reshaping of social roles for visually impaired people. However, there exist barriers to conducting a beauty regime because of the reliance on visual information and color variances in makeup. We present a content analysis of 145 YouTube videos to demonstrate visually impaired individuals' unique practices before, during, and after doing makeup. We then conducted semi-structured interviews with 12 visually impaired people to uncover their perceptions of and challenges with the makeup process. Overall, our findings on unique practices of how visually impaired people do makeup (e.g., how to differentiate makeup products) illuminate challenges in the learning process, task completion, and acquisition of feedback. The existing challenges provide opportunities for future research to address learning barriers, the lack of sufficient feedback, and inaccessible product details, making the experience of doing makeup more accessible to people with visual impairments.},
  author_notes={(*: equal contribution)},
  pdf={CHI2022_BlindCosmetics.pdf},
  selected={true},
}

@inproceedings{huh2022git_diff,
  abbr={CSCL},
  title = {{A Duoethnographic Study of a Mixed-Ability Team In a Collaborative Group Programming Project}},
  author    = {Huh, Mina and Seo, Jooyoung},
  booktitle={Proceedings of the 15th International Conference on Computer-Supported Collaborative Learning-CSCL},
  year={2022},
  abstract={As diversity efforts in computer science education strive to make programming more accessible, we take a step forward by exploring collaborative context. We present a duoethnography of a mixed-ability team collaborating in a group programming project. We (one sighted; one blind) immersed ourselves into a collaborative programming project using a popular social coding platform (i.e., GitHub) and triangulated our self-reflexivity through cross- checking reviews and interviews. By analyzing interaction logs on GitHub, code writing in integrated development environments, and interview notes, we show distinct approaches adopted by the sighted programmer and the programmer with visual impairments, tools and technologies used, and social implications of visual impairment. This research offers reflections on accommodations and technologies provided to support mixed-ability teams collaborating in a group programming project.},
  pdf={CSCL2022_duoethnography.pdf},
  selected={true},
}
@inproceedings{huh2023genassist,
  abbr={UIST},
  title = {{GenAssist: Making Image Generation Accessible}},
  author    = {Huh, Mina and Peng, Yi-Hao and Pavel, Amy},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology 2023 (* Conditional Acceptance)},
  year={2023},
  abstract={Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. 
While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. 
To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. 
Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.},
  pdf={GenAssist-Alt-Text.pdf},
  website={https://minahuh.com/GenAssist/},
  selected={true},
}

@inproceedings{huh2023avscript,
  abbr={CHI},
  title = {{AVscript: Accessible Video Editing with Audio-Visual Scripts}},
  author    = {Huh, Mina and Yang, Saelyne and Peng, Yi-Hao and Chen, Xiang 'Anthony' and Kim, Young-Ho and Pavel, Amy},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  year={2023},
  abstract={Sighted and blind and low vision (BLV) creators alike use videos to communicate with broad audiences. Yet, video editing remains inaccessible to BLV creators. Our formative study revealed that current video editing tools make it difficult to access the visual content, assess the visual quality, and efficiently navigate the timeline. We present AVscript, an accessible text-based video editor. AVscript enables BLV creators to edit their video using a script that embeds the video's visual content, visual errors (e.g., dark or blurred footage), and speech. BLV creators can use AVscript to efficiently navigate between scenes and visual errors or to locate objects in the frame or spoken words of interest. A comparison study (N=12) showed that AVscript significantly lowered BLV creators' mental demands while increasing confidence and independence in video editing. We further demonstrate the potential of AVscript through an exploratory study (N=3) where BLV creators edited their own footage.},
  pdf={AVscript-tagged.pdf},
  website={https://minahuh.com/AVscript/},
  selected={true},
}

