<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mina Huh</title> <meta name="author" content="Mina Huh"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://minarainbow.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" target="_blank" href="https://minarainbow.github.io/assets/pdf/Huh_Mina_CV.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Mina</span> Huh </h1> <p class="desc"></p> <h5 class="post-description"> Ph.D. student @ <a href="https://www.cs.utexas.edu/" rel="external nofollow noopener" target="_blank">UT Austin Computer Science</a> </h5> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic3-1400.webp"></source> <img src="/assets/img/prof_pic3.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="Mina is sitting on a white desk, facing the camera with a smile on her face. She sits upright with both hands resting on the desk," onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Hi! I am a Ph.D. student at <a href="https://www.cs.utexas.edu/" rel="external nofollow noopener" target="_blank">UT Austin Computer Science</a>, advised by <br> <a href="https://amypavel.com/" rel="external nofollow noopener" target="_blank">Amy Pavel</a>. My research focuses on building AI-powered creativity support tools for efficient and accessible media authoring. My work has been recognized and supported by <a href="https://www.cs.utexas.edu/news/2024/computer-science-doctoral-student-earns-google-fellowship" rel="external nofollow noopener" target="_blank">Google Ph.D. Fellowship</a>.</p> <p>I completed my B.Sc in <a href="https://cs.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">Computer Science</a> at <a href="http://kaist.ac.kr/en/" rel="external nofollow noopener" target="_blank">KAIST</a> where I was advised by Prof. <a href="https://juhokim.com/" rel="external nofollow noopener" target="_blank">Juho Kim</a> as part of <a href="https://www.kixlab.org/" rel="external nofollow noopener" target="_blank">KIXLAB</a>. I have also worked as a research intern at <a href="https://research.adobe.com/research/" rel="external nofollow noopener" target="_blank">Adobe Research</a> and <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab" rel="external nofollow noopener" target="_blank">Naver AI Lab</a>.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 205px"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 11, 2025</th> <td> ✈️ Attending CVPR to present our papers at <a href="https://egovis.github.io/cvpr25/" rel="external nofollow noopener" target="_blank">EgoVis</a>, <a href="https://varworkshop.github.io/" rel="external nofollow noopener" target="_blank">VAR</a>, and <a href="https://sites.google.com/view/cvpr-2025-demodiv/" rel="external nofollow noopener" target="_blank">DemoDiv</a> workshops! </td> </tr> <tr> <th scope="row">May 27, 2025</th> <td> 🏔️ Back at <a href="https://research.adobe.com/research/" rel="external nofollow noopener" target="_blank">Adobe Seattle</a> for the summer! </td> </tr> <tr> <th scope="row">May 20, 2025</th> <td> 🏫 We introduced <a href="https://minahuh.com/AVscript/" rel="external nofollow noopener" target="_blank">AVscript</a> at Texas School for the Blind and Visually Impaired! </td> </tr> <tr> <th scope="row">Jan 17, 2025</th> <td> 📄 My <a href="https://minahuh.com/publications/" rel="external nofollow noopener" target="_blank">Adobe internship paper</a> is accepted to <a href="https://chi2025.acm.org/" rel="external nofollow noopener" target="_blank">CHI 2025</a>! </td> </tr> <tr> <th scope="row">Jan 2, 2025</th> <td> 📄 Our paper on <a href="https://minahuh.com/publications/" rel="external nofollow noopener" target="_blank">long-to-short video creation</a> is accepted to <a href="https://iui.acm.org/2025/" rel="external nofollow noopener" target="_blank">IUI 2025</a>! We combine extractive and abstractive summarization for video editing. </td> </tr> <tr> <th scope="row">Dec 4, 2024</th> <td> ✈️ We will be organizing a workshop at <a href="https://chi2025.acm.org/" rel="external nofollow noopener" target="_blank">CHI 2025</a> on Generative AI and Accessibility. </td> </tr> <tr> <th scope="row">Oct 26, 2024</th> <td> 🏆 Excited to receive <a href="https://www.cs.utexas.edu/news/2024/computer-science-doctoral-student-earns-google-fellowship" rel="external nofollow noopener" target="_blank">Google Ph.D. Fellowship</a> (2024-2026). </td> </tr> <tr> <th scope="row">Oct 21, 2024</th> <td> 👩‍💻 Gave a talk at CMU Accessibility Seminar! </td> </tr> <tr> <th scope="row">Oct 13, 2024</th> <td> ✈️ Served as a student vounteer co-chair for <a href="https://uist.acm.org/2024/" rel="external nofollow noopener" target="_blank">UIST 2024</a>! </td> </tr> <tr> <th scope="row">Jul 10, 2024</th> <td> 📄 Our paper on <a href="https://minahuh.com/publications/" rel="external nofollow noopener" target="_blank">long-form visual question answering</a> is accepted to <a href="https://colmweb.org/" rel="external nofollow noopener" target="_blank">COLM 2024</a>! We will soon release the dataset! </td> </tr> <tr> <th scope="row">Jul 4, 2024</th> <td> 📄 Our paper on accessible web design is accepted to <a href="https://uist.acm.org/2024/" rel="external nofollow noopener" target="_blank">UIST 2024</a>! </td> </tr> <tr> <th scope="row">Jun 28, 2024</th> <td> 🏔️ Started my internship at <a href="https://research.adobe.com/research/" rel="external nofollow noopener" target="_blank">Adobe Seattle</a>! </td> </tr> <tr> <th scope="row">Jan 24, 2024</th> <td> 📄 How can we make short-form videos accessible? Check our <a href="https://minahuh.com/ShortScribe/" rel="external nofollow noopener" target="_blank">ShortScribe</a> at CHI 2024! </td> </tr> <tr> <th scope="row">Oct 24, 2023</th> <td> 🏆 Honored to receive the UIST best paper award for <a href="https://minahuh.com/GenAssist/" rel="external nofollow noopener" target="_blank">GenAssist</a>! Many thanks to my co-authors! </td> </tr> <tr> <th scope="row">Jun 24, 2023</th> <td> 📄 Happy to announce that <a href="https://minahuh.com/GenAssist/" rel="external nofollow noopener" target="_blank">our paper</a> got conditionally accepted to <a href="https://uist.acm.org/2023/" rel="external nofollow noopener" target="_blank">UIST 2023</a>! </td> </tr> <tr> <th scope="row">May 30, 2023</th> <td> 👩‍💻 Will serve as a student volunteer co-chair for <a href="https://uist.acm.org/2023/" rel="external nofollow noopener" target="_blank">UIST 2023</a>! </td> </tr> <tr> <th scope="row">Apr 22, 2023</th> <td> 🇩🇪 Presenting our paper <a href="https://minahuh.com/AVscript/" rel="external nofollow noopener" target="_blank">AVscript</a> at CHI 2023 in Hamburg! </td> </tr> <tr> <th scope="row">Apr 20, 2023</th> <td> 🌉 Attending <a href="https://cra.org/cra-wp/grad-cohort-for-women/" rel="external nofollow noopener" target="_blank">CRA-WP workshop</a> in San Francisco! </td> </tr> <tr> <th scope="row">Jan 16, 2023</th> <td> 📄 Happy to announce that <a href="https://minahuh.com/AVscript/" rel="external nofollow noopener" target="_blank">our paper</a> got conditionally accepted to <a href="https://chi2023.acm.org" rel="external nofollow noopener" target="_blank">CHI 2023</a>! </td> </tr> <tr> <th scope="row">Dec 21, 2022</th> <td> 👩‍💻 Will serve as a web co-chair for <a href="https://assets23.sigaccess.org/" rel="external nofollow noopener" target="_blank">ASSETS 2023</a>! </td> </tr> <tr> <th scope="row">Nov 8, 2022</th> <td> 🏅 Received a special recognition for outstanding reviews at CHI 2023. </td> </tr> <tr> <th scope="row">Oct 29, 2022</th> <td> ✈️ Attending <a href="https://uist.acm.org/uist2022/" rel="external nofollow noopener" target="_blank">UIST 2022</a> in person! 📍Bend, Oregon </td> </tr> <tr> <th scope="row">Oct 23, 2022</th> <td> 👩‍💻 Excited to serve as a student volunteer for <a href="https://assets22.sigaccess.org/" rel="external nofollow noopener" target="_blank">ASSETS 2022</a>!. </td> </tr> <tr> <th scope="row">Sep 15, 2022</th> <td> 🔥 Submitted one paper to CHI 2023! </td> </tr> <tr> <th scope="row">Aug 22, 2022</th> <td> 🤘 Started my Ph.D. program at UT Austin! </td> </tr> <tr> <th scope="row">Mar 8, 2022</th> <td> 📄 Our paper got accepted to <a href="https://2022.isls.org/programs/cscl/" rel="external nofollow noopener" target="_blank">CSCL 2022</a>, checkout the <a href="https://minahuh.com/assets/pdf/CSCL2022_duoethnography.pdf" rel="external nofollow noopener" target="_blank">paper</a>! </td> </tr> <tr> <th scope="row">Feb 28, 2022</th> <td> 🍀 Excited to start my internship at <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab" rel="external nofollow noopener" target="_blank">Naver AI lab</a>. </td> </tr> <tr> <th scope="row">Nov 18, 2021</th> <td> 🤞 Submitted one paper to <a href="https://2022.isls.org/programs/cscl/" rel="external nofollow noopener" target="_blank">CSCL 2022</a>. Good luck! </td> </tr> <tr> <th scope="row">Nov 16, 2021</th> <td> 📄📄 Happy to announce that one paper got conditionally accpeted and one under R&amp;R! <br> Stay tuned for exciting accessibility research at <a href="https://chi2022.acm.org" rel="external nofollow noopener" target="_blank">CHI 2022</a>! </td> </tr> <tr> <th scope="row">Oct 10, 2021</th> <td> 🔥 Excited to serve as the JST captain student volunteer for <a href="https://uist.acm.org/uist2021/" rel="external nofollow noopener" target="_blank">UIST 2021</a>! </td> </tr> <tr> <th scope="row">Sep 17, 2021</th> <td> 🤞 Successfully submitted two first-author papers to <a href="https://chi2022.acm.org" rel="external nofollow noopener" target="_blank">CHI 2022</a>! Fingers crossed! </td> </tr> <tr> <th scope="row">May 10, 2021</th> <td> 👩‍💻 Attending <a href="http://chi2021.acm.org" rel="external nofollow noopener" target="_blank">CHI 2021</a> virtually! Let’s chat! </td> </tr> <tr> <th scope="row">May 5, 2021</th> <td> 👩‍💻 Attending <a href="https://sway.office.com/Dbr2Uz14PIn31Dq8" rel="external nofollow noopener" target="_blank">Microsoft Ability Summit</a>! </td> </tr> <tr> <th scope="row">Jan 22, 2021</th> <td> 📄 Our <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/01/Reimagining-Accessibility-and-Inclusion-in-K12_Making-Higher-Ed-in-CS-Accessible-Group-B.pdf" rel="external nofollow noopener" target="_blank">report</a> “Reimagining Accessibility and Inclusion in K-12 CS Education<br> through Curriculum and Professional Development” is now public! </td> </tr> <tr> <th scope="row">Dec 18, 2020</th> <td> 👩‍💻 Presenting at <a href="https://cscwaws2020.github.io" rel="external nofollow noopener" target="_blank">CSCW Asia Winter School 2020</a>! </td> </tr> <tr> <th scope="row">Dec 15, 2020</th> <td> 📄 Our paper “RubySlippers: Supporting Content-based Voice Navigation for How-to Videos”<br> has beeen conditionally accepted for <a href="https://chi2021.acm.org/" rel="external nofollow noopener" target="_blank">CHI 2021</a>. More details soon! </td> </tr> <tr> <th scope="row">Nov 17, 2020</th> <td> 👩‍💻 Attending <a href="https://www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/" rel="external nofollow noopener" target="_blank">Microsoft Accessible Computer Science Education Fall Workshop</a>! </td> </tr> <tr> <th scope="row">Oct 26, 2020</th> <td> 👩‍💻 Attending <a href="http://assets20.sigaccess.org" rel="external nofollow noopener" target="_blank">ASSETS 2020</a> virtually! </td> </tr> <tr> <th scope="row">Oct 20, 2020</th> <td> 👩‍💻 Excited to serve as a student volunteer for <a href="https://uist.acm.org/uist2020/" rel="external nofollow noopener" target="_blank">UIST 2020</a>! </td> </tr> <tr> <th scope="row">Aug 27, 2020</th> <td> 🏆 Won the grand prize at the <a href="https://soc.kaist.ac.kr/board/view?bbs_id=news&amp;bbs_sn=9352&amp;page=1&amp;skey=subject&amp;svalue=&amp;menu=85" rel="external nofollow noopener" target="_blank">2020 KAIST Undergraduate Research Program</a> </td> </tr> </table> </div> </div> <div class="publications"> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div> <div id="huh2025videodiff" class="col-sm-8"> <div class="title">VideoDiff: Human-AI Video Co-Creation with Alternatives</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Mina Huh</span></strong>, Dingzeyu Li, Kim Pimmel, Valentina Shin, Amy Pavel, and Mira Dontcheva</div> <div class="periodical"> <em>Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/VideoDiff_CamReady.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>To make an engaging video, people sequence interesting moments and add visuals such as B-rolls or text. While video editing requires time and effort, AI has recently shown strong potential to make editing easier through suggestions and automation. A key strength of generative models is their ability to quickly generate multiple variations, but when provided with many alternatives, creators struggle to compare them to find the best fit. We propose VideoDiff, an AI video editing tool designed for editing with alternatives. With VideoDiff, creators can generate and review multiple AI recommendations for each editing process: creating a rough cut, inserting B-rolls, and adding text effects. VideoDiff simplifies comparisons by aligning videos and highlighting differences through timelines, transcripts, and video previews. Creators have the flexibility to regenerate and refine AI suggestions as they compare alternatives. Our study participants (N=12) could easily compare and customize alternatives, creating more satisfying results.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">UIST</abbr></div> <div id="huh2024designchecker" class="col-sm-8"> <div class="title">DesignChecker: Visual Design Support for BLV Web Developers</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Mina Huh</span></strong>, and Amy Pavel</div> <div class="periodical"> <em>Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2407.17681" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://minahuh.com/DesignChecker/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Blind and low vision (BLV) developers create websites to share knowledge and showcase their work. A well-designed website can engage the audience and deliver information effectively, yet it remains challenging for BLV developers to review their web designs. We conducted interviews with BLV developers (N=12) and analyzed 20 websites created by BLV developers. BLV developers created highly accessible websites but wanted to assess the usability of their website for sighted users and follow the design standards of other websites. They also encountered challenges using screen readers to identify illegible text, misaligned elements, and inharmonious colors. We present DesignChecker, a browser extension that helps BLV developers to improve their web design. With DesignChecker, users can assess their current design by comparing it to visual design guidelines, a reference website of their choice, or a trend of similar websites. DesignChecker also identifies the specific HTML elements that violate web design guidelines and suggests CSS changes for improvements. Our user study participants (N=8) all expressed enthusiasm about using DesignChecker in the future to improve their web design.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">COLM</abbr></div> <div id="huh2024lfvqa" class="col-sm-8"> <div class="title">Long-Form Answers to Visual Question from Blind and Low Vision People</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Mina Huh</span></strong>, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, and Amy Pavel</div> <div class="periodical"> <em>Conference on Language Modeling</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <div>💡 <b>Oral Spotlight (Top 2%)</b> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2408.06303" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://minahuh.com/LFVQA/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Vision language models can now generate long-form answers to questions about images – long-form visual question answers (LFVQA). We contribute VizWiz-LF, a dataset of long-form answers to visual questions posed by blind and low vision (BLV) users. VizWiz-LF contains 4.2k long-form answers to visual questions, collected from human expert describers and six VQA models. We develop and annotate functional roles of sentences in LFVQA and demonstrate that long-form answers contain information beyond the answer such as explanations and suggestions. We further evaluate 360 long-form answers with BLV and sighted people to understand their preferences and assess the correctness of the long-form answers. Long-form answers generated by models often hallucinate with incorrect information to unanswerable questions (e.g., blurry images), yet BLV people often perceive these long-form answers as correct. To reduce hallucinations, we evaluate VQA models on their ability to correctly abstain from answering unanswerable questions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">UIST</abbr></div> <div id="huh2023genassist" class="col-sm-8"> <div class="title">GenAssist: Making Image Generation Accessible</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Mina Huh</span></strong>, Yi-Hao Peng, and Amy Pavel</div> <div class="periodical"> <em>Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology 2023</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <div>🏆 <b>Best Paper Award (Top 1%)</b> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/GenAssist-Alt-Text.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://minahuh.com/GenAssist/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Blind and low vision (BLV) creators use images to communicate with sighted audiences. However, creating or retrieving images is challenging for BLV creators as it is difficult to use authoring tools or assess image search results. Thus, creators limit the types of images they create or recruit sighted collaborators. While text-to-image generation models let creators generate high-fidelity images based on a text description (i.e. prompt), it is difficult to assess the content and quality of generated images. We present GenAssist, a system to make text-to-image generation accessible. Using our interface, creators can verify whether generated image candidates followed the prompt, access additional details in the image not specified in the prompt, and skim a summary of similarities and differences between image candidates. To power the interface, GenAssist uses a large language model to generate visual questions, vision-language models to extract answers, and a large language model to summarize the results. Our study with 12 BLV creators demonstrated that GenAssist enables and simplifies the process of image selection and generation, making visual authoring more accessible to all.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CHI</abbr></div> <div id="huh2023avscript" class="col-sm-8"> <div class="title">AVscript: Accessible Video Editing with Audio-Visual Scripts</div> <div class="author"> <strong><span style="color:var(--global-theme-color)">Mina Huh</span></strong>, Saelyne Yang, Yi-Hao Peng, Xiang ’Anthony’ Chen, Young-Ho Kim, and Amy Pavel</div> <div class="periodical"> <em>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/AVscript-tagged.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://minahuh.com/AVscript/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Sighted and blind and low vision (BLV) creators alike use videos to communicate with broad audiences. Yet, video editing remains inaccessible to BLV creators. Our formative study revealed that current video editing tools make it difficult to access the visual content, assess the visual quality, and efficiently navigate the timeline. We present AVscript, an accessible text-based video editor. AVscript enables BLV creators to edit their video using a script that embeds the video’s visual content, visual errors (e.g., dark or blurred footage), and speech. BLV creators can use AVscript to efficiently navigate between scenes and visual errors or to locate objects in the frame or spoken words of interest. A comparison study (N=12) showed that AVscript significantly lowered BLV creators’ mental demands while increasing confidence and independence in video editing. We further demonstrate the potential of AVscript through an exploratory study (N=3) where BLV creators edited their own footage.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%69%6E%61%68%75%68@%63%73.%75%74%65%78%61%73.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=Rt3lwtYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/minarainbow" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/mina-huh-422946194" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/mina1004h" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://youtube.com/@minahuh6146" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fab fa-youtube"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Mina Huh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>